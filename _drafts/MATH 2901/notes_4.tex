\include{config/config}

\begin{document}
% ========== Edit your name here
\title{MATH 2901 Basic Probability Lecture Notes 4}
\author{Instructor: Richard Kleeman}
\date{}
\maketitle

%\medskip

% ========== Contents begin here ==============
\section{Expectation of discrete random variables}
The intuition of expectation is the \emph{average value} of an experiment. Suppose we do an experiment for $N$ repeated times. The probability of each possible outcome $x$ can be approximately defined by 
\begin{equation*}
    \Prob(x) \approx f(x) = \frac{freq(x)}{N}.
\end{equation*}
Then the average outcome is
\begin{equation*}
    m \approx \frac{1}{N}\sum_{x} freq(x)x = \sum_x f(x)x
\end{equation*}

\begin{definition}
The \textbf{mean value}, or \textbf{expectation}, or \textbf{expected value} of the random variable $X$ with mass function $f$ is defined to be 
\begin{equation*}
    \Exp(X) = \sum_{x:f(x)>0} xf(x)
\end{equation*}
whenever this sum is absolutely convergent.
\end{definition}

\begin{remark}
\begin{enumerate}[(a)]
    \item For notation convenience, we also write $\Exp(X) = \sum_x xf(x)$.
    \item We require \textbf{absolute convergence} in order that $\Exp(X)$ be unchanged by reordering the $x_i$. 
\end{enumerate}
\end{remark}

\begin{theorem}[Riemann Rearrangement Theorem] \href{https://en.wikipedia.org/wiki/Riemann_series_theorem}{See this: Riemann series theorem}.
\end{theorem}

\begin{lemma}
If $X$ has mass function $f$ and $g:\R\to\R$, then
\begin{equation*}
    \Exp(g(x)) = \sum_x g(x) f(x)
\end{equation*}
whenever this sum is absolutely convergent.
\end{lemma}

\begin{example}
If $X$ is a random variable with mass function f, and $g(x) = x^2$, then 
\begin{equation*}
    \Exp(X^2) = \sum_x g(x)f(x) = x^2 f(x).
\end{equation*}
\end{example}

\begin{definition}
If $k$ is a positive integer, the $k$th \text{moment} $m_k$ of $X$ is defined to be 
\begin{equation*}
    m_k = \Exp(X^k).
\end{equation*}
The $k$th \textbf{central moment} $\sigma_k$ is defined as
\begin{equation*}
    \sigma_k = \Exp\left( (X-m_1)^k \right) = \Exp\left( (X-E(X))^k \right).
\end{equation*} 
\end{definition}

The two moments of most use are $m_1 = \Exp(X)$ and $\sigma_2 = \Exp( (X - \Exp(X))^2$, called the \textbf{mean} (or \textbf{expectation)} and \textbf{variance} of $X$. These two quantities are measures of the mean and dispersion of $X$; that is, $m_1$ is the average value of $X$, and $\sigma_2$ measures the amount by which $X$ tends to deviate from this average. The mean $m_1$ is often denoted $\mu$, and the variance of $X$ is often denoted $\Var(X)$. The positive square root $\sigma = \Var(X)$ is called the \textbf{standard deviation}, and in this notation $\sigma_2 = \sigma^2$. 

The central moments $\{\sigma_i\}$ can be expressed in terms of the ordinary moments $\{m_i\}$. For example, $\sigma_1 = 0$, and 
\begin{equation*}
    \begin{aligned} 
        \sigma_{2} &=\sum_{x}\left(x-m_{1}\right)^{2} f(x) \\ 
        &=\sum_{x} x^{2} f(x)-2 m_{1} \sum_{x} x f(x)+m_{1}^{2} \sum_{x} f(x) \\ 
        &=m_{2}-m_{1}^{2} ,
    \end{aligned}
\end{equation*}
which may be written as 
\begin{equation}
    \label{eq:4-1}
    \tag{4-1}
    \Var(X) = \Exp\left( (X-E(X))^2 \right) = \Exp(X^2) - \Exp(X)^2.
\end{equation}

\begin{example}[Binomial variables]
Let $X$ be a random variable with binomial distribution. The p.m.f. is 
\begin{equation*}
    f(k) = \binom{n}{k} p^k q^{n-k} \quad k = 0,\dots, n,
\end{equation*}
where $q = 1-p$. The expectation of $X$ is
\begin{equation*}
    \Exp(X) = \sum_{k=0}^n k f(k) = \sum_{k=0}^n k\binom{n}{k} p^k q^{n-k}.
\end{equation*}
We use the following algebraic identity to compute $\Exp(X)$.
\begin{equation}
    \label{eq:4.2}
    \tag{4-2}
    \sum_{k=0}^n \binom{n}{k} x^k = (1+x)^n, 
\end{equation}
Differentiate it and multiply by $x$, we obtain 
\begin{equation}
    \label{eq:4.3}
    \tag{4-3}
    \sum_{k=0}^n k \binom{n}{k} x^k = nx(1+x)^{n-1}. 
\end{equation}
We substitute $x = p / q$ to obtain $\Exp(X) = np$. A similar argument shows that the variance of $X$ is given by $\Var(X) = npq$. 
\end{example}

We can think of the process of calculating expectations as a \textbf{linear operator} on the space of random variables. 

\begin{theorem}
The expectation operator $\Exp$ has the following properties: 
\begin{enumerate}[(a)]
    \item if $X\geq 0$, then $\Exp(X) \geq 0$,
    \item if $a, b \in \R$, then $\Exp(aX+bY) = a\Exp(X) + b\Exp(Y)$,
    \item the random variable 1, taking the value 1 always, has expectation $\Exp(1) = 1$. 
\end{enumerate}
\end{theorem}

\begin{proof}
We only prove the second property, which is also called the linear property.

We must use the joint p.m.f. of $X$ and $Y$ to compute the expectation. 
\begin{equation*}
    \begin{split}
        \Exp(aX+bY) &= \sum_{i, j} (ax_i + by_j) f(x_i, y_j) \\
        &= a \sum_{i,j} x_i f(x_i, y_j) + b\sum_{i,j} y_j f(x_i, y_j) \\
        &= a\sum_{i} x_i f_X(x_i) + b\sum_{j}y_j f_Y(y_j) \\ 
        &= a\Exp(X) + b\Exp(Y),
    \end{split}
\end{equation*}
where $f_X(x)$ and $f_Y(y)$ are marginal p.m.f. of $X$ and $Y$ respectively.
\end{proof}

\begin{caution}
It is \textbf{NOT} in general true that $\Exp(XY)$ is the same as $\Exp(X)\Exp(Y)$. 
\end{caution}

\begin{lemma}
If $X$ and $Y$ are independent, then $\Exp(XY) = \Exp(X)\Exp(Y)$. 
\end{lemma}
\begin{proof}
If $X, Y$ are independent, $f(x,y) = f_X(x) f_Y(y)$. Then
\begin{equation*}
    \Exp(XY) = \sum_{ij} x_i y_j f(x,y) = \sum_{i} \left( x_i f_X(x_i) \right) \sum_{j} \left( y_j f_Y(y_j) \right) = \Exp(X) \Exp(Y).
\end{equation*}
\end{proof}

\begin{definition}
$X$ and $Y$ are called \textbf{uncorrelated} if $\Exp(XY) = \Exp(X)\Exp(Y)$.
\end{definition}

\begin{caution}
Independent variables are uncorrelated. But the converse is \textbf{NOT} true.
\end{caution}

\begin{theorem}
For random variables $X$ and $Y$, 
\begin{enumerate}[(a)]
    \item $\Var(aX) = a^2 \Var(X)$ for $a \in \R$,
    \item $\Var(X+Y) = \Var(X) + \Var(Y)$ is $X$ and $Y$ are uncorrelated.
\end{enumerate}
\end{theorem}

\begin{remark}
The above theorem shows that the variance operator $\Var$ is \textbf{NOT} a linear operator, even when it 
is applied only to uncorrelated variables. 
\end{remark}

%==============================================
Sometimes the sum $S = \sum xf(x)$ does not converge absolutely, which means the mean of the distribution does not exist. Here is an example. 
\begin{example}
\textbf{A distribution without a mean.} Let $X$ have mass function 
\begin{equation*}
    f(k) = Ak^{-1} \quad k = \pm 1, \pm 2, \dots,
\end{equation*}
where $A$ is chosen so that $\sum_k f(k) = 1$. The sum $\sum_k kf(k) = A\sum_{k\neq 0} k^{-1}$ doesn't converge absolutely, because both the positive and the negative parts diverge. 
\end{example}
This example is suitable to point out that we can base probability theory upon the expectation operator $\Exp$ rather than upon the probability measure $\Prob$. Roughly speaking, the way we proceed is to postulate axioms, such as (a)-(c) of the above Theorem, for a so-called ``expectation operator" $\Exp$ acting on a space of ``random variables". The probability of an event can then be recaptured by defining $\Prob(A) = \Exp(I_A)$.  

Recall the indicator function of a set $A$ is defined as
\begin{equation*}
    I_A(\omega) = \begin{cases} 1 & \omega \in A, \\
    0 & \omega \not\in A. \end{cases}
\end{equation*}
In addition, we have $\Exp(I_A) = \Prob(A)$.

%==============================================

\section{Dependence of discrete random variables}
\begin{definition}
The \textbf{joint distribution function} $F:\R^2 \to [0,1]$ of $X$ and $Y$, where $X$ and $Y$ are discrete variables, is given by 
\begin{equation*}
    F(x, y) = \Prob(X\leq x \text{ and } Y \leq y). 
\end{equation*}
Their \textbf{joint mass function} $f:\R^2 \to [0,1]$ is given by 
\begin{equation*}
    f(x,y) = \Prob(X = x \text{ and } Y = y). 
\end{equation*}
\end{definition}

We write $F_{X,Y}$ and $f_{X,Y}$ when we need to stress the role of $X$ and $Y$. We may think of the joint mass function in the following way. If $A_x = \{X = x\}$ and $B_y = \{Y = y\}$, then 
\begin{equation*}
    f(x,y) = \Prob(A_x \cap B_y).
\end{equation*}

\begin{lemma}
The discrete random variables $X$ and $Y$ are \textbf{independent} if and only if 
\begin{equation}
    \label{eq:4.4}
    \tag{4-4}
    f_{X,Y}(x,y) = f_X(x)f_Y(y) \quad \forall x,y \in \R.
\end{equation}
More generally, $X$ and $Y$ are independent if and only if $f_{X,Y}(x, y)$ can be \textbf{factorized as the product} $g(x)h (y)$ of a function of $x$ alone and a function of $y$ alone. 
\end{lemma}
\begin{remark}
We stress that the factorization \eqref{eq:4.4} must hold for all $x$ and $y$ in order that $X$ and $Y$ be independent. 
\end{remark}

\begin{lemma}
\begin{equation*}
    \Exp(g(X, Y))=\sum_{x, y} g(x, y) f_{X, Y}(x, y).
\end{equation*}
\end{lemma}

\begin{definition}
The \textbf{covariance} of $X$ and $Y$ is
\begin{equation*}
    \cov(X,Y) = \Exp\left( (X-\Exp(X))(Y-\Exp(Y)) \right).
\end{equation*}
The \textbf{correlation (coefficient)} of $X$ and $Y$ is 
\begin{equation*}
\corr(X, Y) = \rho(X,Y) = \frac{\cov(X,Y)}{\sqrt{\Var(X)\Var(Y)}}
\end{equation*}
as long as the variances are non-zero. 
\end{definition}

\begin{remark}
Notice the following two equations.
\begin{enumerate}
    \item $\cov(X,X) = \Var(X)$,
    \item $\cov(X,Y) = \Exp(XY) - \Exp(X)\Exp(Y)$.
\end{enumerate}
\end{remark}

Covariance itself is not a satisfactory measure of dependence because the scale of values which $\cov(X, Y)$ may take contains no points which are clearly interpretable in terms of the relationship between $X$ and $Y$.

\begin{theorem}[Cauchy-Schwarz inequality] For random variables $X$ and $Y$, 
\begin{equation*}
    \Exp(XY)^2 \leq \Exp(X^2) \Exp(Y^2)
\end{equation*} 
with equality if and only if $\Prob(aX = bY) = 1$ for some real $a$ and $b$, at least one of which is non-zero. 
\end{theorem}

\begin{proof}
For $a, b \in \R$, let $Z = aX - bY$. Then 
\begin{equation*}
    0 \leq \Exp(Z^2) = a^2 \Exp(X^2) - 2ab\Exp(XY) + b^2\Exp(Y^2).
\end{equation*}
Thus the right-hand side is a quadratic in the variable $a$ with at most one real root. Its discriminant must be non-positive. That is to say, if $b \neq 0$, 
\begin{equation*}
    \Exp(XY)^2 - \Exp(X^2) \Exp(Y^2) \leq 0. 
\end{equation*}
The discriminant is zero if and only if the quadratic has a real root. This occurs if and only if 
\begin{equation*}
    \Exp\left( (aX-bY)^2 \right) = 0
\end{equation*}
for some $a$ and $b$.
\end{proof}

We define $X' = X-\Exp(X), Y' = Y - \Exp(Y)$. Since all $X, Y$ satisfy the Cauchy-Schwarz inequality, so do $X'$ and $Y'$. Therefore, 
\begin{equation*}
    \Exp(X'Y')^2 \leq \Exp(X'^2) \Exp(Y'^2) \quad \Leftrightarrow \quad \cov(X, Y)^2 \leq \Var(X)\Var(Y).
\end{equation*}
Therefore, 
\begin{equation*}
    \rho(X,Y)^2 \leq 1 \quad \Rightarrow \quad \rho(X,Y) \in [-1, 1].
\end{equation*}
which gives the following lemma.

\begin{lemma}
The correlation coefficient $\rho$ satisfies $\abs{\rho (X, Y) } \leq 1$ with equality if and only if $\Prob(aX + bY = c) = 1$ for some $a, b, c \in \R$. 
\end{lemma}


\section{Expectation of continuous random variables}
\subsection{Idea of translating expectation from discrete to continuous}
Suppose we have a continuous random variable $X$ with $f$ being the probability density function. We split $X$ into small intervals $\Delta x$. Then $p_i = f(x_i)\Delta x$. $\frac{p_i}{\Delta x}$ is an approximation of probability density function. Therefore, 
\begin{equation*}
    \Exp(X) \approx \sum_{i} x_i p_i = \sum_{i} x_i f(x_i) \Delta x,
\end{equation*}
which is the Remann sum. We take the limit and get
\begin{equation*}
    \Exp(x) = \int_{-\infty}^\infty x f(x)dx.
\end{equation*}

\subsection{Expectation}
\begin{definition}
The \textbf{expectation} of a continuous random variable $X$ with density function $f$ is given by 
\begin{equation*}
    \Exp(X) = \int_{-\infty}^\infty xf(x) dx
\end{equation*}
whenever this integral exists.
\end{definition}

\begin{theorem}
If $X$ and $g(X)$ are continuous random variables, then \begin{equation*}
    \Exp\left( g(X) \right) = \int_{-\infty}^\infty g(x)f(x) dx.
\end{equation*}
\end{theorem}

\begin{definition}
The $k$th \textbf{moment} of a continuous variable $X$ is defined as
\begin{equation*}
    \Exp(X^k) = \int_{-\infty}^\infty x^k f(x) dx
\end{equation*}
whenever the integral converges.
\end{definition}

\begin{example}[Cauchy distribution] The random variable $X$ has the Cauchy distribution t if it has density 
function 
\begin{equation*}
    f(x) = \frac{1}{\pi (1+x^2)}, \quad x \in \R.
\end{equation*}
This distribution is notable for having \textbf{no moments}.
\end{example}

\section{Dependence of continuous random variables}
\begin{definition}
The \textbf{joint distribution function} of $X$ and $Y$ is the function $F: \R^2 \to [0, 1]$ given by 
\begin{equation*}
    F(x,y) = \Prob(X\leq x, Y \leq y).
\end{equation*}
\end{definition}

\begin{definition}
The random variables $X$ and $Y$ are \textbf{(jointly) continuous} with \textbf{joint (probability) density function} $f : \R^2 \to [0, \infty)$ if
\begin{equation*}
    F(x, y)=\int_{v=-\infty}^{y} \int_{u=-\infty}^{x} f(u, v) d u d v \quad \text{for each } x, y\in\R. 
\end{equation*}
If $F$ is sufficiently differentiable at the point $(x , y)$, then we usually specify 
\begin{equation*}
    f(x, y)=\frac{\partial^{2}}{\partial x \partial y} F(x, y).
\end{equation*}
\end{definition}

\begin{newnotion}{Probabilities}
\begin{equation*}
    \begin{aligned} 
        \Prob(a \leq X \leq b, c \leq Y \leq d) &=F(b, d)-F(a, d)-F(b, c)+F(a, c) \\ 
        &=\int_{y=c}^{d} \int_{x=a}^{b} f(x, y) d x d y. \end{aligned}
\end{equation*}
If $B$ is a sufficiently nice subset of $\R^2$, then
\begin{equation*}
    \Prob \left( (X, Y) \in B \right)=\iint_{B} f(x, y) d x d y.
\end{equation*}
\end{newnotion}
\begin{newnotion}{Marginal distributions}
The marginal distribution functions of $X$ and $Y$ are
\begin{equation*}
    F_{X}(x)=\Prob(X \leq x)=F(x, \infty), \quad F_{Y}(y)=\Prob(Y \leq y)=F(\infty, y). 
\end{equation*}
\begin{equation*}
    F_{X}(x)=\int_{-\infty}^{x}\left(\int_{-\infty}^{\infty} f(u, y) d y\right) d u.
\end{equation*}
Marginal density function of $X$ and $Y$:
\begin{equation*}
    f_{X}(x)=\int_{-\infty}^{\infty} f(x, y) d y, \quad f_{Y}(y)=\int_{-\infty}^{\infty} f(x, y) d x.
\end{equation*}
\end{newnotion}

\begin{newnotion}{Expectation}
If $g: \R^2 \to \R$ is a sufficiently nice function, then 
\begin{equation*}
    \Exp(g(X, Y))=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y) f(x, y) d x d y;
\end{equation*}
in particular, setting $g(x, y) = ax + by$, 
\begin{equation*}
    \Exp(aX+bY) = a\Exp(X) + b\Exp(Y).
\end{equation*}
\end{newnotion}

\begin{newnotion}{Independence}
The random variables $X$ and $Y$ are independent if and only if 
\begin{equation*}
    F(x,y) = F_X(x) F_Y(y) \quad \forall x, y \in \R,
\end{equation*}
which, for \textbf{continuous random variables}, is equivalent to requiring that 
\begin{equation*}
    f(x,y) = f_X(x) f_Y(y).
\end{equation*}
\end{newnotion}
 
\begin{theorem}[Cauchy-Schwarz inequality] For any pair $X, Y$ of jointly continuous variables, we have that 
\begin{equation*}
    \Exp(XY)^2 \leq \Exp(X^2) \Exp(Y^2), 
\end{equation*}
with equality if and only if $\Prob(aX = bY) = 1$ for some real $a$ and $b$, at least one of which is non-zero. 
\end{theorem}


\end{document}